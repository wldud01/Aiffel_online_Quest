{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a50d3c",
   "metadata": {},
   "source": [
    "## 번역가는 대화에도 능하다 [프로젝트]\n",
    "\n",
    "- 챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다.\n",
    "- 과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다.\n",
    "- 주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05754732",
   "metadata": {},
   "source": [
    "Step 1. 데이터 다운로드\n",
    "- https://github.com/songys/Chatbot_data\n",
    "- 읽어 온 데이터의 질문과 답변을 각각 questions, answers 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd411a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9635a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = os.getenv(\"HOME\") + '/aiffel/Aiffel_online_Quest/GoingDeeper06'\n",
    "df = pd.read_csv(path+'/ChatbotData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023d6953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11823 entries, 0 to 11822\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       11823 non-null  object\n",
      " 1   A       11823 non-null  object\n",
      " 2   label   11823 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 277.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1307d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거\n",
    "duplicated_q = list(df[df['Q'].duplicated()].index)\n",
    "duplicated_q += list(df[df['A'].duplicated()].index)\n",
    "df = df.drop(set(duplicated_q), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "022afa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 12시 땡!\n",
      "answer 하루가 또 가네요.\n"
     ]
    }
   ],
   "source": [
    "question = df['Q'].values\n",
    "answer = df['A'].values\n",
    "\n",
    "\n",
    "print(\"question\",question[0])\n",
    "print(\"answer\", answer[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca899b8c",
   "metadata": {},
   "source": [
    "Step 2. 데이터 정제\n",
    "-  소문자 변환\n",
    "- 정규식을 활용해서 특수문자 제외\n",
    "- 중복되는 문장은 데이터에서 제외 ( 위에서 이미 제외 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3928a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복제거 \n",
    "# 데이터 전처리 함수 재정의\n",
    "import re\n",
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    # 소문자, 특수 문자 제외\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"[?.!,]+\", \"\", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    #sentence = re.sub(r\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣?.!,]+\", \" \", sentence)\n",
    "    \n",
    "    return sentence.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9043c9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하루가 또 가네요\n",
      "12시 땡\n"
     ]
    }
   ],
   "source": [
    "question = [preprocess_sentence(q) for q in question]\n",
    "answer = [preprocess_sentence(a) for a in answer]\n",
    "\n",
    "\n",
    "print(answer[0])\n",
    "print(question[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b693e4b",
   "metadata": {},
   "source": [
    "Step 3. 데이터 토큰화\n",
    "- KoNLPy 사용\n",
    "- build_corpus() 함수를 구현\n",
    "    - 소스 문장 데이터와 타겟 문장 데이터를 입력\n",
    "    - 데이터를 앞서 정의한 preprocess_sentence() 함수로 정제\n",
    "    - 토큰화는 전달받은 토크나이즈 함수를 사용합니다. 이번엔 mecab.morphs\n",
    "    - 토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "58c75bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "def build_corpus(question, answer):\n",
    "    # 데이터 정제\n",
    "    ques = [preprocess_sentence(q) for q in question]\n",
    "    ans = [preprocess_sentence(a) for a in answer]\n",
    "    \n",
    "    # 토크나이저로 토큰화\n",
    "    mecab = Mecab()\n",
    "    tokenized_question, tokenized_answer = [], []\n",
    "    for q, a in zip(ques, ans):\n",
    "        tokenized_question.append(mecab.morphs(q))\n",
    "        tokenized_answer.append(mecab.morphs(a))\n",
    "\n",
    "    # 길이 필터링\n",
    "    filtered_question = [q for q in tokenized_question if len(q) < 50 and len(q)>2]\n",
    "    filtered_answer = [a for a, q in zip(tokenized_answer, tokenized_question) if len(q) < 50 and len(q)>2]\n",
    "    \n",
    "    return filtered_question, filtered_answer\n",
    "    \n",
    "# 추상적인 사고 == 노이즈 == 창의력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "064b9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_data,a_data = build_corpus(question,answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "4e0ba90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7378\n",
      "7378\n"
     ]
    }
   ],
   "source": [
    "print(len(q_data))\n",
    "print(len(a_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "58a72526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', '시', '땡']\n",
      "['하루', '가', '또', '가', '네요']\n"
     ]
    }
   ],
   "source": [
    "print(q_data[0])\n",
    "print(a_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d48bc5",
   "metadata": {},
   "source": [
    "Step 4. Augmentation\n",
    "-  Lexical Substitution을 실제로 적용\n",
    "-  한국어로 사전 훈련된 Embedding 모델을 다운로드\n",
    "- Korean (w) 가 Word2Vec으로 학습한 모델이며 용량도 적당하므로 사이트에서 Korean (w)를 찾아 다운로드하고, ko.bin 파일 , 다운로드한 모델을 활용해 데이터를 Augmentation \n",
    "- Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록, 이후엔 반대로 원본 que_corpus 와 Augmentation된 ans_corpus 가 병렬을 이루도록 하여 전체 데이터가 원래의 3배가량으로 늘어나도록 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "2678e85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.3 in /opt/conda/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2cc4a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import random\n",
    "\n",
    "def load_model(model_path):\n",
    "\n",
    "    wv_model = gensim.models.Word2Vec.load(model_path)\n",
    "   # wv_model= gensim.models.Word2Vec.load(model_path)\n",
    "    return wv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "642286ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다']\n"
     ]
    }
   ],
   "source": [
    "print(question[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "a41b3cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, wv_model):\n",
    "    \"\"\"\n",
    "    문장을 입력받아 Lexical Substitution을 수행하는 함수\n",
    "\n",
    "    :param sentence: 대체를 수행할 문장\n",
    "    :param wv_model: gensim Word2Vec 모델\n",
    "    :return: 대체된 문장\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    sub_token = random.choice(tokens)\n",
    "    #print(tokens)\n",
    "    #print(sub_token)\n",
    "    try:\n",
    "        similar_word = wv_model.most_similar(sub_token)[0][0]\n",
    "    except KeyError:\n",
    "        similar_word = sub_token\n",
    "\n",
    "    augmented_sentence = ' '.join([similar_word if token == sub_token else token for token in tokens])\n",
    "    #print(augmented_sentence)\n",
    "    return augmented_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "70a1f9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/Aiffel_online_Quest/GoingDeeper06/ko.bin\n",
      "12시 끗\n",
      "12시 땡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_138/3238749839.py:14: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  similar_word = wv_model.most_similar(sub_token)[0][0]\n"
     ]
    }
   ],
   "source": [
    "model_path = os.getenv('HOME') + '/aiffel/Aiffel_online_Quest/GoingDeeper06/ko.bin'\n",
    "print(model_path)\n",
    "wv_model = load_model(model_path)\n",
    "print(lexical_sub(question[0],wv_model))\n",
    "print(question[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "c12bea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/Aiffel_online_Quest/GoingDeeper06/ko.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d40304341c42eda9c273dbc13a5c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_138/3238749839.py:14: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  similar_word = wv_model.most_similar(sub_token)[0][0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be0d89938f142f8ba0aa01f41b6aca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['하루가 각기 가네요', '위로해 드립니다', '여행은 언제나 좋죠', '눈살이 찌푸려지죠', '다시 새로이 사는 게 마음 편해요', '많이 모르고 있을 수도 있어요', '시간을 정하고 해보세요', '자랑하는 자리니까요', '그 사람도 그럴 거예요', '혼자를 즐기세요']\n",
      "['12시 끗', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', 'ppl 심하네', 'sd카드 망가졌어', 'sns 맞팔 과연 안하지ㅠㅠ', 'sns 시간낭비인 거 아는데 매일 하는 중', 'sns보면 나만 빼고 다 행복해보여', '이따금 궁금해', '가끔은 혼자인게 좋다']\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 모델 파일 경로 (예: 'ko.bin')\n",
    "model_path = os.getenv('HOME') + '/aiffel/Aiffel_online_Quest/GoingDeeper06/ko.bin'\n",
    "print(model_path)\n",
    "wv_model = load_model(model_path)\n",
    "\n",
    "# 예제 문장\n",
    "new_corpus_q = []\n",
    "new_corpus_a = []\n",
    "\n",
    "for q in tqdm(question):\n",
    "    new_src = lexical_sub(q, wv_model)\n",
    "    if new_src is not None: \n",
    "        new_corpus_q.append(new_src)\n",
    "    # Augmentation이 없더라도 원본 문장을 포함시킵니다\n",
    "    else: new_corpus_q.append(q)\n",
    "\n",
    "for a in tqdm(answer):\n",
    "    new_src = lexical_sub(a, wv_model)\n",
    "    if new_src is not None: \n",
    "        new_corpus_a.append(new_src)\n",
    "    # Augmentation이 없더라도 원본 문장을 포함시킵니다\n",
    "    else: new_corpus_a.append(a)\n",
    "\n",
    "#augmented_sentence = lexical_sub(sentence, wv_model)\n",
    "print(new_corpus_a[:10])\n",
    "print(new_corpus_q[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "68c95e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', '시', '끗']\n",
      "['하루', '가', '각기', '가', '네요']\n"
     ]
    }
   ],
   "source": [
    "q_data_aug, a_data_aug = build_corpus(new_corpus_q,new_corpus_a)\n",
    "print(q_data_aug[0])\n",
    "print(a_data_aug[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9afb0",
   "metadata": {},
   "source": [
    "Step 5. 데이터 벡터화\n",
    "- 타겟 데이터인 ans_corpus 에 <start> 토큰과 <end> 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화\n",
    "- 특수 토큰을 더함으로써 ans_corpus 또한 완성이 되었으니, que_corpus 와 결합하여 전체 데이터에 대한 단어 사전을 구축하고 벡터화하여 enc_train 과 dec_train 을 얻으세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "face3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token(ans_corpus):\n",
    "    return [\"<start>\"] + ans_corpus + [\"<end>\"]\n",
    "\n",
    "a_data = [add_token(a) for a in a_data]\n",
    "new_corpus_a = [add_token(a) for a in a_data_aug]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7b94059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_corpus = a_data + new_corpus_a +new_corpus_a\n",
    "que_corpus = q_data + new_corpus_q+q_data\n",
    "\n",
    "ans_cor = ans_corpus[:20000]\n",
    "que_cor = que_corpus[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "209cf083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(len(ans_corpus[:20000]))\n",
    "print(len(que_corpus[:20000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "79d50409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,   # pad token의 일련번호\n",
    "                       bos_id=1,  # 문장의 시작을 의미하는 bos token(<s>)의 일련번호\n",
    "                       eos_id=2,  # 문장의 끝을 의미하는 eos token(</s>)의 일련번호\n",
    "                       unk_id=3):   # unk token의 일련번호\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "f6753a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 시 땡\n",
      "<start> 하루 가 또 가 네요 <end>\n"
     ]
    }
   ],
   "source": [
    "que_cor = [' '.join(q) for q in que_cor]\n",
    "ans_cor = [' '.join(a) for a in ans_cor]\n",
    "\n",
    "print(que_cor[0])\n",
    "print(ans_cor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "91ef016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=././kor-kor_corpus.txt --model_prefix=kor-kor_spm --vocab_size=3000--pad_id==0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ././kor-kor_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: kor-kor_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 3000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespac"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "es: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ././kor-kor_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 40000 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=1042591\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9503% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1021\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999503\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 40000 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 4040 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 40000\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 6584\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 6584 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=3758 obj=9.85442 num_tokens=12350 num_tokens/piece=3.28632\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=3037 obj=9.14477 num_tokens=12352 num_tokens/piece=4.06717\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: kor-kor_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: kor-kor_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 3000\n",
    "tokenizer = generate_tokenizer(que_cor + ans_cor, VOCAB_SIZE, 'kor-kor')\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")  # 문장 양 끝에 <s> , </s> 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "c122a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size:  1000\n",
      "\n",
      "\n",
      "Train Example: 20000\n",
      ">> 12 시 땡\n",
      ">> 가족 들 보 고 싶 어\n",
      ">> 갑자기 물어봐서 당황 했 어\n",
      ">> 개 좋 아\n",
      ">> 게으른 동료 가 있 어\n",
      "\n",
      "\n",
      "Test Example: 1000\n",
      ">> 이별 너무 힘드 네 ㅠ\n",
      ">> 이 별 한 달 반 째\n",
      ">> 이별 후 후회 아쉬움\n",
      ">> 이별 당시 보다 더 짠 한 건 나 떠났 으면 잘 이나 살 지\n",
      ">> 이별 은 진짜 비겁 한 짓 같 아\n"
     ]
    }
   ],
   "source": [
    "test_sentence_count = int(len(que_cor)*0.05)\n",
    "print(\"Test Size: \", test_sentence_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "#train_que_cor = que_cor[:-test_sentence_count]\n",
    "test_que_cor = que_cor[-test_sentence_count:]\n",
    "print(\"Train Example:\", len(que_cor))\n",
    "for sen in que_cor[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n",
    "print(\"Test Example:\", len(test_que_cor))\n",
    "for sen in test_que_cor[0:100][::20]: \n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "560193de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수\n",
    "def make_corpus(sentences, tokenizer):\n",
    "    corpus = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = tokenizer.encode_as_ids(sentence)\n",
    "        corpus.append(tokens)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "38d3b0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a650ece1971492d88f17612700b3bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95507cd5cdaf468ab2a72676577ee86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 영어와 스페인어를 각각 토큰화 해줍니다. 훈련 데이터만 토큰화를 하고, 같은 토크나이저를 사용한다는 점\n",
    "eng_corpus = make_corpus(que_cor, tokenizer)\n",
    "spa_corpus = make_corpus(ans_cor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "2cdd988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 330, 1882, 119, 1851, 2]\n",
      "\n",
      "\n",
      "[1, 5, 6, 0, 8, 10, 0, 4, 440, 18, 199, 18, 54, 5, 11, 7, 9, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 잘 됐는지 확인\n",
    "print(eng_corpus[0])\n",
    "print('\\n')\n",
    "print(spa_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "0ebf3d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1  330 1882  119 1851    2    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "[  1   5   6   0   8  10   0   4 440  18 199  18  54   5  11   7   9   4\n",
      "   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 50\n",
    "enc_ndarray = tf.keras.preprocessing.sequence.pad_sequences(eng_corpus, maxlen=MAX_LEN, padding='post')\n",
    "dec_ndarray = tf.keras.preprocessing.sequence.pad_sequences(spa_corpus, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "print(enc_ndarray[0])\n",
    "print(dec_ndarray[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "59371c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_ndarray, dec_ndarray)).batch(batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1896b28",
   "metadata": {},
   "source": [
    "Step 6. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "01087963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    # 각 모델에 홀수 짝수에 각각 정현파 함수를 적용한다 sin, cos\n",
    "    # 위치 별 벡터 구하는 공식\n",
    "    def cal_angle(position,i):\n",
    "        # postion, index\n",
    "        return position / np.power(10000,(2*(i//2) / np.float32(d_model)))\n",
    "    \n",
    "    def get_posi_engle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    # 모든 토큰에 position 값을 계산하고 나서 array로 만든다\n",
    "    sinusoid_table = np.array([get_posi_engle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    # 짝수 - sin , 홀수 - cos\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "fc4b419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask  생성하기\n",
    "def generate_padding_mask(seq):\n",
    "    # seq == 0 이면 float32로 바꾸고\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "3523ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        # 각각 Dense layer를 거쳐서 상태가 있는 형태로 만든다\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        # Q K V 값을 동일하게 num head 만큼 나눠준다\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "\n",
    "        # 그리고 attention layer를 거친다.\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "7894566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "61bf026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        # 잔차\n",
    "        residual = x\n",
    "        # 정규화\n",
    "        out = self.norm_1(x)\n",
    "        # self attention\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        # dropout\n",
    "        out = self.do(out)\n",
    "        # residual \n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        # 잔차\n",
    "        residual = out\n",
    "        # attention에서 나온 값 layer norm\n",
    "        out = self.norm_2(out)\n",
    "        # feed forward net\n",
    "        out = self.ffn(out)\n",
    "        # drop out\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "82eb5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        # 첫번 째 self attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        # encoder의 Value 값으로 Attention\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        # feed forward net\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "3680e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "59d48afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e4831a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "93c5cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d7f8cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "77fa7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "b196f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "0759fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "f1e982f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8978ea93aa49fbbed68fdafd055f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.4108\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b90c3c5de54986bd90b092d4c33a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 0.3918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe019dd568c45b599c738c7c46c6f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 0.3793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8035722440e64e6fa866269d06918a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss 0.3460\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f955f22b544123b9c3fbc041e1331c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss 0.2956\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9b56ce6868450f8be98fb2485b4e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 0.2475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84939ea425364362bc48e4e8578135c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss 0.2181\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d345e28d9449b3b47d220b35be0ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss 0.1904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3689406de3704fc693779b0ee381bfa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss 0.1693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6746a4eb4e55450d85e451d5316526c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss 0.1520\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e160f3114d4b56963e5c5738da1773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss 0.1376\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f644535bb645d08745608f68830e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss 0.1280\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ecb02fa0754452a515eab56d665332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss 0.1171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcefb94683f4ac491f9114d1141ca67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss 0.1078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641af24635274b8d8b36afc73c4adc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss 0.0980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42fcad0a301453fa81113603d46f940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss 0.0922\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd9cf174a58474bbb763eacccf2603e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss 0.0867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d642d6db5fd434793b18a6054662d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss 0.0819\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf43970b159449b99dae448859553428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss 0.0770\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db11e8421124540bdbffd32dc1435db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss 0.0713\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8db853214848a2ba0f428dd2ac9daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Loss 0.0695\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db90a645cc64518a3a325bfa735eca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Loss 0.0632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665a7b5eafb14ddf89df59e781ffde4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Loss 0.0621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f5380e608c40b38bbe8cb0ab2e46eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Loss 0.0594\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7808af50c94eef9dde43c6ffc863b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Loss 0.0583\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93300179afbd41d8a079a56bfff41697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Loss 0.0533\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025b2f8ec067487dac99950e8e42f91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Loss 0.0531\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655629cccea54426a30d5afd3f7967c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Loss 0.0494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704449bb57f9466cb97892c11ddf3a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Loss 0.0497\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecbace6135b4fc1beef20f2d4d4afc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Loss 0.0476\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(enumerate(train_dataset), total=dataset_count)\n",
    "    for (batch, (src, tgt)) in tqdm_bar:\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        # 여기서 tqdm_bar.set_description을 사용하여 에폭과 배치 손실을 출력할 수 있습니다.\n",
    "        tqdm_bar.set_description(f\"Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}\")\n",
    "\n",
    "    # 에폭마다 평균 손실을 계산합니다.\n",
    "    total_loss /= dataset_count\n",
    "    print(f\"Epoch {epoch + 1} Loss {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1493e3",
   "metadata": {},
   "source": [
    "Step 7. 성능 측정하기\n",
    "- 챗봇의 경우, 올바른 대답을 하는지가 중요한 평가 지표\n",
    "- BLEU Score를 계산하는 calculate_bleu() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "f8ca642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: ['하루가', '참', '길어요']\n",
      "대답: ['하루', '가', '참', '길', '어', '요']\n",
      "BLEU Score: 1.1640469867513693e-231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "# 아래 두 문장을 바꿔가며 테스트 해보세요\n",
    "reference = \"하루가 참 길어요\".split()\n",
    "candidate = \"하루 가 참 길 어 요\".split()\n",
    "\n",
    "print(\"질문:\", reference)\n",
    "print(\"대답:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "6e4453e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.16666666666666669\n",
      "BLEU-2: 0.02\n",
      "BLEU-3: 0.025\n",
      "BLEU-4: 0.03333333333333333\n",
      "\n",
      "BLEU-Total: 0.040824829046386304\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "d60759a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(tokens, model, src_tokenizer, tgt_tokenizer):\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=MAX_LEN,\n",
    "                                                           padding='post')\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)   \n",
    "    for i in range(MAX_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(padded_tokens, output)\n",
    "\n",
    "        predictions, _, _, _ = model(padded_tokens, \n",
    "                                      output,\n",
    "                                      enc_padding_mask,\n",
    "                                      combined_mask,\n",
    "                                      dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)  \n",
    "            return result\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)  \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "79fbfc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    src_tokens = src_tokenizer.encode_as_ids(src_sentence)\n",
    "    tgt_tokens = tgt_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    if (len(src_tokens) > MAX_LEN): return None\n",
    "    if (len(tgt_tokens) > MAX_LEN): return None\n",
    "\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = translate(src_tokens, model, src_tokenizer, tgt_tokenizer).split()\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", ' '.join(candidate))\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "886d124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  가난 한 자 의 설움\n",
      "Model Prediction:  <sssnsns 종교 문제 가 되 기 힘들 죠 <end>\n",
      "Real:  ['<start>', '돈', '은', '다시', '들어올', '거', '예요', '<end>']\n",
      "Score: 0.023980\n",
      "\n",
      "-----------------------------------------------------\n",
      "Source Sentence:  가만 있 어도 땀 난다\n",
      "Model Prediction:  <ssssssns 끊 미리 충전 도 되 어 봐요 <end>\n",
      "Real:  ['<start>', '땀', '을', '식혀', '주', '세요', '<end>']\n",
      "Score: 0.023980\n",
      "\n",
      "-----------------------------------------------------\n",
      "Source Sentence:  가상 화폐 쫄딱 망함\n",
      "Model Prediction:  <sssssss 에너찬 가 필요 한 순간 이 네요 <end>\n",
      "Real:  ['<start>', '어서', '잊', '고', '새', '출발', '하', '세요', '<end>']\n",
      "Score: 0.023980\n",
      "\n",
      "-----------------------------------------------------\n",
      "Source Sentence:  가스 불 켜 고 나갔 어\n",
      "Model Prediction:  <sssssn 자책 하 지 마세요 <end>\n",
      "Real:  ['<start>', '빨리', '집', '에', '돌아가', '서', '끄', '고', '나오', '세요', '<end>']\n",
      "Score: 0.017742\n",
      "\n",
      "-----------------------------------------------------\n",
      "Source Sentence:  가스 비 너무 많이 나왔 다\n",
      "Model Prediction:  <sssssnsns 검색 하 지 마세요 <end>\n",
      "Real:  ['<start>', '다음', '달', '에', '는', '더', '절약', '해봐요', '<end>']\n",
      "Score: 0.024762\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "# Q. 인덱스를 바꿔가며 테스트해 보세요\n",
    "test_idx = range(10,15)\n",
    "\n",
    "for test in test_idx:\n",
    "    eval_bleu_single(transformer, \n",
    "                     que_cor[test], \n",
    "                     ans_cor[test], \n",
    "                     tokenizer, \n",
    "                     tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "cb39dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "858f69c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bac6db6471e42c48cf628ca4bdc4574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 100\n",
      "Total Score: 0.08239454782465133\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, que_cor[:100], ans_cor[:100], tokenizer, tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffbdcb8",
   "metadata": {},
   "source": [
    "### 회고\n",
    "- 지난 과정에 이어서 트랜스포머로 번역기를 만들어보았다.생각보다 전처리 부분에서 시간이 많이 걸리고 중요하다는 것을 느꼈다.\n",
    "- 데이터 증강과 BLUE에 대해서 자세히는 모르고 있었는데 배울 수 있는 좋은 프로젝트였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90284e50",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
